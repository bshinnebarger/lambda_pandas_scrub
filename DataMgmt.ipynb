{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import urllib.request\n",
    "import re\n",
    "import os\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "\n",
    "import data_utils\n",
    "import aws_utils\n",
    "\n",
    "# You may want to do it this way if you are debuging / editing the support files \n",
    "# modules imported via aimport will before you execute any cell\n",
    "# %load_ext autoreload\n",
    "# %autoreload 1\n",
    "# %aimport data_utils\n",
    "# %aimport aws_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we'll establish some basic logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "s_tdy = datetime.today().strftime('%m-%d-%Y')\n",
    "main_log = Path(f'logs/chi-town-scrub-data_{s_tdy}.log')\n",
    "if not main_log.parent.exists(): main_log.parent.mkdir(parents=True, exist_ok=True)\n",
    "if main_log.exists(): os.remove(main_log)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch_main_log = logging.FileHandler(main_log)\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch_main_log.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s')\n",
    "\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "ch_main_log.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(ch_main_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download our dataset\n",
    "\n",
    "### We'll be using Chicago crime statistics from 2001 - present (approx 1.8 GB) as an example.  To find out more about the dataset you can check out [data.gov](https://catalog.data.gov/dataset/crimes-2001-to-present-398a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_DIR = Path('data')\n",
    "DATA_URL = 'https://data.cityofchicago.org/api/views/ijzp-q8t2/rows.csv?accessType=DOWNLOAD'\n",
    "\n",
    "if not DOWNLOAD_DIR.exists(): DOWNLOAD_DIR.mkdir(parents=True, exist_ok=True)\n",
    "tgt_file = DOWNLOAD_DIR/'chi-town-crime.csv'\n",
    "\n",
    "if tgt_file.exists():\n",
    "    logger.info(f'{tgt_file.name} already exists')\n",
    "else:\n",
    "    logger.info('Attempting to download dataset')\n",
    "    urllib.request.urlretrieve(DATA_URL, tgt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's split our data up into manageable chunks\n",
    "\n",
    "### Our example data could be easily handled in memory, but imagine we're working with 100 gigs or a TB, in that case we can divide and conquer\n",
    "\n",
    "By default the helper function data_utils.split_file will split things into files of 1,000,000 rows each.  This is useful if you want to look at it in something in Excel.  Some considerations for processing files in Lambda would be that we can have a max of approx 3 gigs of memory for a Lambda function, so you would want to keep file sizes < 1 gig, to leave room for overhead, depending on how you process it.  Also, Lambda functions can run up to a max of 15 minutes.  We can run a  thousand of a particular Lambda function at once by default (you can request limit increases).   Lambda functions also get more processing power when you give them more memory, but the exact formula for this isn't public knowledge AFAIK, so play around with it and try to find a balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_file = DOWNLOAD_DIR/'chi-town-crime.csv'\n",
    "\n",
    "if not tgt_file.exists():\n",
    "    logger.info(f'Couldn\"t find {tgt_file.name}, try downloading it again above')\n",
    "else:\n",
    "    split_files = data_utils.split_file(tgt_file, has_headers=True, include_headers=True)\n",
    "\n",
    "logger.info('Split files')\n",
    "for file in split_files:\n",
    "    logger.info(f'{file.name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's upload our chunks of data to the S3 bucket we created in the CloudFormation script\n",
    "\n",
    "<font color=red>Change S3_BUCKET below to the name of your bucket</font>\n",
    "\n",
    "Here I am using the s3fs module for the uploads.  I've had issues when transferring larger files (> 10 gigs or so) with timeouts, and also Sagemaker notebooks periodically request you to log-in again, so if you're transferring a large amount of files, you may want to opt for the command line\n",
    "    \n",
    "To do that, open up the terminal (click the + button on the top left to get to the luancher, then select terminal at the bottom)\n",
    "\n",
    "Do the following:<br>\n",
    "    \n",
    "cd Sagemaker/data<br>\n",
    "aws s3 cp . s3://YOUR_BUCKET --recursive --exclude '*crime.csv'<br>\n",
    "\n",
    "you may want to run it this way first<br>\n",
    "aws s3 cp . s3://YOUR_BUCKET --recursive --dryrun --exclude '*crime.csv'<br>\n",
    "    \n",
    "the parameter --dryrun will show you what it's going to do without actually doing it, so you can make sure you got the command right\n",
    "\n",
    "You can see more about [the aws s3 cp (copy) command here](https://docs.aws.amazon.com/cli/latest/reference/s3/cp.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_BUCKET = 'chi-town-scrub-data'\n",
    "s3 = s3fs.S3FileSystem()\n",
    "\n",
    "if not s3.exists(S3_BUCKET):\n",
    "    logger.info(f'Couldn\"t find S3 bucket {S3_BUCKET}, make sure the bucket above exists in your account')\n",
    "    logger.info(f'It should be the one you created in the CloudFormation script')\n",
    "else:\n",
    "    for file in sorted(DOWNLOAD_DIR.glob('*[0-9][0-9][0-9]*.csv')):\n",
    "        s3_bucket_key = f'{S3_BUCKET}/{file.name}'\n",
    "        aws_utils.upload_s3_file(s3, file, s3_bucket_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
