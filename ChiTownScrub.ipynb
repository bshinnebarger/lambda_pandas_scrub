{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, namedtuple, Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import s3fs\n",
    "\n",
    "pd.set_option('display.max_columns', 50)\n",
    "\n",
    "import data_utils\n",
    "import aws_utils\n",
    "\n",
    "# You may want to do it this way if you are debuging / editing the support files \n",
    "# modules imported via aimport will before you execute any cell\n",
    "# %load_ext autoreload\n",
    "# %autoreload 1\n",
    "# %aimport data_utils\n",
    "# %aimport aws_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we'll establish some basic logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create main logger\n",
    "logger = logging.getLogger('main')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "s_tdy = datetime.today().strftime('%m-%d-%Y')\n",
    "main_log = Path(f'logs/chi-town-scrub_{s_tdy}.log')\n",
    "if not main_log.parent.exists(): main_log.parent.mkdir(parents=True, exist_ok=True)\n",
    "if main_log.exists(): os.remove(main_log)\n",
    "\n",
    "# create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch_main_log = logging.FileHandler(main_log)\n",
    "ch.setLevel(logging.DEBUG)\n",
    "ch_main_log.setLevel(logging.DEBUG)\n",
    "\n",
    "# create formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(funcName)s - %(message)s')\n",
    "\n",
    "# add formatter to ch\n",
    "ch.setFormatter(formatter)\n",
    "ch_main_log.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(ch_main_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our regex patterns for columns that need to be validated, as well as any general patterns that might be helpful\n",
    "\n",
    "We are going to load our data as str type into a dataframe, so that no automatic type conversions take place, then \n",
    "use regex patterns to validate the expected content of various columns, rejecting or replacing where necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ScrubRE = namedtuple('ScrubRE', ['TWO_LETTERS',\n",
    "                                 'BLOCK', 'IUCR', 'PRIMARY_TYPE', 'DESCRIPTION', 'LOCATION_DESCRIPTION', \n",
    "                                 'LOCATION', 'ZIP_CODES'])\n",
    "\n",
    "# Generic and column specific regex patterns for validation and transformation purposes\n",
    "MY_REGX = ScrubRE(TWO_LETTERS=re.compile(r'^[a-z]{2}$', flags=re.I),                      \n",
    "                      # Look for some kind of address like 013XX and then a street location like W 3RD AVE\n",
    "                      # Will use to validate and to extract\n",
    "                      BLOCK=re.compile(r'^(\\d{1,4}X{1,4}) ((?:[a-z\\d] ?){1,20}){1,5}$', flags=re.I),\n",
    "                      # IUCR we just want to confirm it's some 4 length alphanumeric code\n",
    "                      IUCR=re.compile(r'^[a-z\\d]{4}$', flags=re.I),\n",
    "                      # Look for up to five groups of letters, dashes\n",
    "                      PRIMARY_TYPE=re.compile(r'^(?:[a-z\\-]{1,20}(?: |$)){1,5}$', flags=re.I),\n",
    "                      # Look for up to seven groups of letters, numbers, or [-/:,()$]\n",
    "                      DESCRIPTION=re.compile(r'^(?:[a-z\\-\\/\\:\\,\\.\\(\\)\\d\\$}]{1,25}(?: |$)){1,7}$', flags=re.I),\n",
    "                      # Look for up to seven groups of letters, or [-/.,()]\n",
    "                      LOCATION_DESCRIPTION=re.compile(r'^(?:[a-z\\-\\/\\.\\,\\(\\)]{1,20}(?: |$)){1,7}$', flags=re.I),\n",
    "                      LOCATION=re.compile(r'^\\((-?\\d+\\.\\d+), ?(-?\\d+\\.\\d+)\\)$'),\n",
    "                      # Zip codes should be length 4 or 5\n",
    "                      ZIP_CODES=re.compile(r'^\\d{5}|\\d{4}$'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following two functions define how we want to process our dataset\n",
    "\n",
    "### First, we focus on columns that can't be null or if they fail some validation, need to be excluded entirely\n",
    "### Second, we focus on everything else\n",
    "\n",
    "This is where we will pre-process, validate, post-process and / or generate any derived columns\n",
    "\n",
    "<font color=blue>__The following two functions rely heavily on data_utils.process_field to do the heavy lifting, so you should review what we can do with that function__</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hard_rejects(df):\n",
    "    '''\n",
    "    Here we define any logic based on columns that can't be null or for which a validation error would mean we need to exclude the entire row\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): our raw data\n",
    "    \n",
    "    Returns:\n",
    "    df_filt (DataFrame): a DataFrame where any hard rejects have been excluded entirely\n",
    "    hard_rejects (dict of column name, indices): column names and row indices of anything that can't be null or failed some validation and\n",
    "                                                 had to be excluded entirely becase of it\n",
    "    '''\n",
    "    df_filt = df.copy()\n",
    "    \n",
    "    logger.info(f'Length of records before hard rejects {len(df_filt):,}')\n",
    "    \n",
    "    hard_rejects = defaultdict(set)\n",
    "\n",
    "    # Column specific logic\n",
    "    # id -> Not null, must be digits\n",
    "    id_val = lambda series: series.str.isdigit()\n",
    "    # case_number -> Not null, first two must be alpha\n",
    "    case_number_val = lambda series: series.str.slice(0,2).str.match(MY_REGX.TWO_LETTERS)\n",
    "    \n",
    "    # date -> Not null, try to parse known formats, also want to extract year, month\n",
    "    def date_yr_mo(df, dates):\n",
    "        df['year'] = dates.dt.year.astype(str)\n",
    "        df['month'] = dates.dt.month.astype(str)\n",
    "\n",
    "    nonnull_fields = {}\n",
    "    nonnull_fields['id'] = {'validation': id_val}\n",
    "    nonnull_fields['case_number'] = {'validation': case_number_val}\n",
    "    nonnull_fields['date'] = {'date_field': True, 'other_nulls': ['0000-00-00'], 'generated_cols': [date_yr_mo]}\n",
    "\n",
    "    for field, params in nonnull_fields.items():\n",
    "        logger.debug(f'Processing {field}')\n",
    "        data_utils.process_field(df_filt, field, hard_rejects, **params)\n",
    "\n",
    "    # Filter out hard rejects\n",
    "    for _, ids in hard_rejects.items():\n",
    "        if len(ids) > 0: \n",
    "            df_filt = df_filt.loc[~df_filt.index.isin(ids)]\n",
    "\n",
    "    logger.info(f'Length of records after hard rejects {len(df_filt):,}')\n",
    "    \n",
    "    return df_filt, hard_rejects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_soft_rejects(df_filt):\n",
    "    '''\n",
    "    Here we define any logic based on columns that can be null, and for which a validation error would mean we set the column to null\n",
    "    \n",
    "    After processing, the data in the original column names will have clean data in them, and columns like '[col_name]_orig' will have the \n",
    "    original value if it failed validation.  See data_utils.process_field for a detailed explanation\n",
    "    \n",
    "    Parameters:\n",
    "    df_filt (DataFrame): our data after processing hard rejects (modified in place)\n",
    "    \n",
    "    Returns:\n",
    "    soft_rejects (dict of column name, indices): column names and row indices of anything that failed some validation and\n",
    "                                                 had to be set to null because of it\n",
    "    '''\n",
    "    soft_rejects = defaultdict(set)\n",
    "\n",
    "    # Generic cosmetic changes (post-processing)\n",
    "    capitalize_first = lambda series: series.str.title()\n",
    "    upper_case = lambda series: series.str.upper()\n",
    "\n",
    "    # Block we want to extract the hidden house number from the street location so we can analyze crime by street\n",
    "    def block_num_addr(df, blocks):\n",
    "        df[['house_num', 'street_addr']] = blocks.str.extract(MY_REGX.BLOCK, expand=True)\n",
    "\n",
    "    primary_type_post = [(None, capitalize_first)]\n",
    "    # Combine regex validation and max length 50\n",
    "    description_val = lambda series: series.str.match(MY_REGX.DESCRIPTION, na=False) & series.str.len().le(50)\n",
    "    description_post = [(None, capitalize_first)]\n",
    "    # Combine regex validation and max length 50\n",
    "    location_description_val = lambda series: series.str.match(MY_REGX.LOCATION_DESCRIPTION, na=False) & series.str.len().le(50)\n",
    "    location_description_post = [(None, capitalize_first)]\n",
    "    # arrest / domestic fields look to be all True / False, so let's just confirm this with a valid values constraint\n",
    "    tf_valid = ['true', 'false']\n",
    "    # beat, district, ward, community area, look to be all integer fields, so for these four, we will \n",
    "    # validate it's an int\n",
    "    valid_int = lambda series: series.str.isdigit()\n",
    "\n",
    "    # Location, we want to extract lat / lon into their own columns\n",
    "    # Also, we don't care about the original field after the extract, so we'll drop it\n",
    "    def location_lat_lon(df, locations):\n",
    "        df[['latitude','longitude']] = locations.str.extract(MY_REGX.LOCATION, expand=True)\n",
    "\n",
    "    # For zip codes we want to prefix 4 length zip codes with a '0' at the beginning after validation\n",
    "    def zip_to_five(zips):\n",
    "        condlist = [zips.str.len().eq(4), zips.str.len().eq(5)]\n",
    "        choicelist = ['0' + zips, zips]\n",
    "        return pd.Series(index=zips.index, data=np.select(condlist, choicelist, default=np.nan))\n",
    "\n",
    "    post_zip_codes = [(None, lambda series: zip_to_five(series))]\n",
    "\n",
    "    nullable_fields = {}\n",
    "    nullable_fields['block'] = {'validation': MY_REGX.BLOCK, 'generated_cols': [block_num_addr]}\n",
    "    nullable_fields['iucr'] = {'validation': MY_REGX.IUCR}\n",
    "    nullable_fields['primary_type'] = {'validation': MY_REGX.PRIMARY_TYPE, 'post_process': primary_type_post}\n",
    "    nullable_fields['description'] = {'validation': description_val, 'post_process': description_post}\n",
    "    nullable_fields['location_description'] = {'validation': location_description_val, 'post_process': location_description_post}\n",
    "    nullable_fields['arrest'] = {'valid_values': tf_valid}\n",
    "    nullable_fields['domestic'] = {'valid_values': tf_valid}\n",
    "    nullable_fields['beat'] = {'validation': valid_int}\n",
    "    nullable_fields['district'] = {'validation': valid_int}\n",
    "    nullable_fields['ward'] = {'validation': valid_int}\n",
    "    nullable_fields['community_area'] = {'validation': valid_int}\n",
    "    nullable_fields['location'] = {'validation': MY_REGX.LOCATION, 'generated_cols': [location_lat_lon], 'drop_field': True}\n",
    "    nullable_fields['zip_codes'] = {'validation': MY_REGX.ZIP_CODES, 'post_process': post_zip_codes}\n",
    "\n",
    "    for field, params in nullable_fields.items():\n",
    "        logger.debug(f'Processing {field}')\n",
    "        data_utils.process_field(df_filt, field, soft_rejects, **params)\n",
    "        \n",
    "    return soft_rejects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's test our scrubbing process on a single file\n",
    "\n",
    "First, we'll get a list of the files we want to process via Lambda\n",
    "\n",
    "<font color=red>Make sure to change the name of the S3_BUCKET to your bucket</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem()\n",
    "FILE_PATTERN = re.compile('.*?(\\d+)\\.csv')\n",
    "S3_BUCKET = 'chi-town-scrub-data'\n",
    "\n",
    "s3_files = aws_utils.get_s3_files_to_process(s3, FILE_PATTERN, S3_BUCKET, '')\n",
    "s3_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick one of the files above to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "s3_bucket_key = s3_files[0][1]\n",
    "print(f'Loading {s3_bucket_key}')\n",
    "with s3.open(s3_files[0][1], 'r') as file:\n",
    "    df = pd.read_csv(file, low_memory=False, encoding='utf-8', dtype=str)\n",
    "\n",
    "# I like column names lower case and with spaces replaced with underscores\n",
    "data_utils.clean_col_names(df)\n",
    "\n",
    "keep_cols = ['id', 'case_number', 'date', 'block', 'iucr', 'primary_type', 'description', 'location_description',\n",
    "            'arrest', 'domestic', 'beat', 'district', 'ward', 'community_area', 'location', 'zip_codes']\n",
    "\n",
    "# Filter out the data to the columns we're interested in\n",
    "df = df[keep_cols]\n",
    "\n",
    "# This will trim data as well as replace multiple white spaces inside text with a single white space\n",
    "data_utils.remove_excess_white_space(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We're not focused too much in this example on exploring the data, but it's useful to take a quick peak at the data in the columns to get an idea of it, and to help visualize what the regex is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of crimes: {len(df):,}')\n",
    "print(f'Number of crimes resulting in arrest: {len(df.arrest.eq(\"true\")):,}')\n",
    "\n",
    "print('\\nMost common crimes (primary type):\\n')\n",
    "crime_cnt = Counter(df.primary_type)\n",
    "\n",
    "for primary_type, cnt in crime_cnt.most_common(5):\n",
    "    print(f'{primary_type} => {cnt:,}')\n",
    "\n",
    "crime_cnt_spec = Counter(df['primary_type'] + ' => ' + df['description'])\n",
    "print('\\nMost common crimes (primary type => description):\\n')\n",
    "for specific_type, cnt in crime_cnt_spec.most_common(10):\n",
    "    print(f'{specific_type} => {cnt:,}')\n",
    "    \n",
    "crime_loc = Counter(df.location_description)\n",
    "print('\\nMost common crime location:\\n')\n",
    "for location, cnt in crime_loc.most_common(10):\n",
    "    print(f'{location} => {cnt:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's clean the data\n",
    "\n",
    "This is essentially what we want the Lambda function to do for us\n",
    "\n",
    "1. Process hard rejects (and filter our DataFrame)\n",
    "2. Process soft rejects\n",
    "3. Analyze and log the rejects (so we know where to focus improvements in our scrubbing regex)\n",
    "4. Upload the reject data to s3, as well as our clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df_filt, hard_rejects = process_hard_rejects(df)\n",
    "soft_rejects = process_soft_rejects(df_filt)\n",
    "unq_hard_rejects, unq_soft_rejects = data_utils.analyze_hard_and_soft_rejects(hard_rejects, soft_rejects)\n",
    "\n",
    "file_name = s3_bucket_key.split('/')[-1]\n",
    "hard_rej_df, soft_rej_df = data_utils.generate_reject_dfs(df, df_filt, file_name, hard_rejects, unq_hard_rejects, soft_rejects, unq_soft_rejects)\n",
    "\n",
    "with s3.open(f'{S3_BUCKET}/rejects/hard_rejects_{file_name}', 'w') as hard_up, \\\n",
    "        s3.open(f'{S3_BUCKET}/rejects/soft_rejects_{file_name}', 'w') as soft_up, \\\n",
    "        s3.open(f'{S3_BUCKET}/clean_data/clean_{file_name}', 'w') as clean_up:\n",
    "    hard_rej_df.to_csv(hard_up, index_label='file_index', encoding='utf-8')\n",
    "    soft_rej_df.to_csv(soft_up, index_label='file_index', encoding='utf-8')\n",
    "    \n",
    "    clean_cols = [col for col in df_filt.columns if '_orig' not in col]\n",
    "    df_filt[clean_cols].to_csv(clean_up, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue>You can go check your S3 bucket now to see the data is there</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a peek at our hard rejcts and soft rejects\n",
    "\n",
    "You will probably be looking these, tweaking your process, and re-running the above code over and over until your happy\n",
    "with the results, then set up your Lambda function to do the heavy lifting\n",
    "\n",
    "Notice that we are storing the file_name of the rejects, and that we uploaded this to s3.  We can easily list all the rejects\n",
    "files in the rejects bucket we made in S3, load them all and concatenate the results to a single DataFrame and analyze everything\n",
    "after Lambda does it's work\n",
    "\n",
    "We are also storing the column(s) that caused the reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_rej_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For example, we stated as a hard reject validation in our regex that all \"valid\" case numbers, start with two letters, so let's check our rejects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_rej_df.loc[hard_rej_df.cols.str.contains('case_number'), 'case_number'].str.slice(0,2).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And for the \"clean\" data\n",
    "df_filt['case_number'].str.slice(0,2).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft rejects, as they are not totally excluded, include the original value, so you can see what is being set to null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_rej_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similar to the function we used above, analyze_hard_and_soft_rejects, we can look at them this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_cols = [col for col in soft_rej_df.columns if '_orig' in col]\n",
    "soft_rej_df[orig_cols].notnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see what's being set to null and then tweak our regex\n",
    "\n",
    "Here, it looks like our description validation is probably too strict, so there's room for improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_REGX.DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_rej_df.loc[soft_rej_df.description_orig.notnull(), ['description', 'description_orig']].sample(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's also useful to look at the \"clean\" DataFrame, and sample it for the columns you need to clean up and see if anything weird is still in there and tweak your regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filt[clean_cols].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's turn this into a Lambda!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
